# Web Scraping com SQLite

Este projeto realiza Web Scraping em pÃ¡ginas da WikipÃ©dia e armazena os dados coletados em um banco de dados SQLite. 
A aplicaÃ§Ã£o coleta informaÃ§Ãµes como tÃ­tulo da pÃ¡gina, conteÃºdo principal, URL, data da postagem e data da coleta.

## ğŸ“Œ Funcionalidades

- **Coleta de dados** de pÃ¡ginas da WikipÃ©dia.
- **Armazena os dados** coletados em um banco SQLite.
- **Busca** por palavra-chave no tÃ­tulo dos posts.
- **Filtra** posts pela data de coleta.
- **Lista** todos os posts coletados.

## ğŸ› ï¸ Tecnologias Utilizadas

- Python 3.x
- SQLite3
- Requests
- BeautifulSoup4

## ğŸ“‚ Estrutura do Projeto

```
ğŸ“ web_scraper_project/
â”‚-- ğŸ“„ main.py            # Script principal para rodar a coleta de dados
â”‚-- ğŸ“„ scrape.py          # MÃ³dulo responsÃ¡vel pelo Web Scraping
â”‚-- ğŸ“„ database.py        # MÃ³dulo de conexÃ£o e manipulaÃ§Ã£o do banco de dados
â”‚-- ğŸ“„ requirements.txt   # DependÃªncias do projeto
â”‚-- ğŸ“„ README.md          # DocumentaÃ§Ã£o do projeto
```

## ğŸš€ Como Executar

### 1ï¸âƒ£ Clonar o repositÃ³rio

```sh
git clone https://github.com/seuusuario/seurepositorio.git
cd seurepositorio
```

### 2ï¸âƒ£ Criar um ambiente virtual (opcional, mas recomendado)

```sh
python -m venv venv
source venv/bin/activate  # Para Linux/Mac
venv\Scripts\activate   # Para Windows
```

### 3ï¸âƒ£ Instalar as dependÃªncias

```sh
pip install -r requirements.txt
```

### 4ï¸âƒ£ Executar o script

```sh
python main.py
```

## ğŸ” Consultas ao Banco de Dados

VocÃª pode buscar dados no banco utilizando os seguintes mÃ©todos:

- **Buscar todos os posts coletados**

```python
from database import get_all_posts
print(get_all_posts())
```

- **Buscar posts por palavra-chave**

```python
from database import search_posts_by_keyword
print(search_posts_by_keyword("PlayStation"))
```

- **Buscar posts por data de coleta**

```python
from database import get_posts_by_collection_date
print(get_posts_by_collection_date("2025-03-18"))
```

## ğŸ“œ ExplicaÃ§Ã£o dos Arquivos

### 1ï¸âƒ£ ```database.py``` (Gerenciamento do Banco de Dados)

Este arquivo contÃ©m funÃ§Ãµes para criar o banco de dados, salvar e realizar consultas:

- ```create_database()```: Cria o banco de dados ```scrapeDB.db``` e a tabela ```posts```.
- ```save_to_database(post_data)```: Salva um post coletado no banco de dados.
- ```get_all_posts()```: Retorna posts que contÃªm a palavra-chave no tÃ­tulo.
- ```get_posts_by_collection_date(date)```: Retorna posts coletados em uma data especÃ­fica.

### 2ï¸âƒ£ ```scrape.py``` (Coletando Dados da Wikipedia)

A funÃ§Ã£o ```metodoScrape(url) coleta os seguintes dados da pÃ¡gina:

- TÃ­tulo: Obtido da tag ```<span class='mw-page-title-main'>```
- Data da postagem: ExtraÃ­do do rodapÃ© da Wikipedia (```footer-info-lastmod```)
- ConteÃºdo: ExtraÃ­do dos parÃ¡grafos dentro do ```bodyContent```
- Data de Coleta: Data e hora da execuÃ§Ã£o do script

### 3ï¸âƒ£ ```main.py``` (ExecuÃ§Ã£o do Programa)

- Cria o banco de dados
- Percorre uma lista de URLs da Wikipedia
- Executa o scraping e salva os dados no banco
- Exibe os posts coletados e permite consultas por palavra-chave e data


## ğŸ“œ Exemplo de SaÃ­da

Ao rodar ```main.py```, a saÃ­da serÃ¡ semelhante a:

```sh
Coletando dados da URL: https://en.wikipedia.org/wiki/PlayStation_5
Coletando dados da URL: https://en.wikipedia.org/wiki/Brazil
Coletando dados da URL: https://en.wikipedia.org/wiki/Nintendo
Coletando dados da URL: https://en.wikipedia.org/wiki/FIFA

Todos os posts coletados:
(1, 'PlayStation 5', 'https://en.wikipedia.org/wiki/PlayStation_5', 'November 12, 2020', '2025-03-18 14:30:00')
(2, 'Brazil', 'https://en.wikipedia.org/wiki/Brazil', 'August 6, 2022', '2025-03-18 14:30:00')
...
```

## ğŸ“± Banco de dados Relacional (SQLite)

Para este projeto, optei por utilizar um banco de dados **relacional (SQL)** em vez de um banco nÃ£o relacional(NoSQL) ou um simples armazenamento em arquivo por alguns motivos essenciais:

### 1ï¸âƒ£ EstruturaÃ§Ã£o e Integridade dos Dados

Como os dados coletados possuem um formato bem definido (tÃ­tulo, conteÃºdo, URL, data de postagem e data de coleta), um banco relacional permite organizÃ¡-los de maneira estruturada em tabelas, garantindo consistÃªncia e integridade.
Em um banco NoSQL (como MongoDB), os dados seriam armazenados como documentos JSON, o que pode gerar inconsistÃªncias caso as estruturas variem ao longo do tempo.

### 2ï¸âƒ£ Facilidade de Consulta e ManipulaÃ§Ã£o 

Bancos relacionais utilizam SQL (Structured Query Language), uma linguagem poderosa para realizar consultas complexas de maneira eficiente.
No projeto, hÃ¡ a necessidade de buscar posts por palavras-chave e filtrar por data de coleta, algo que pode ser feito de forma otimizada com SQL e Ã­ndices, sem necessidade de processar dados manualmente em cÃ³digo.

### 3ï¸âƒ£ Relacionamento e Expansibilidade

Embora o projeto atualmente tenha uma Ãºnica tabela (posts), um banco de dados relacional permite expansibilidade futura, possibilitando a criaÃ§Ã£o de tabelas relacionadas.
Se no futuro for necessÃ¡rio armazenar categorias, autores ou outros metadados dos posts, um modelo relacional facilitarÃ¡ a estruturaÃ§Ã£o e a recuperaÃ§Ã£o dos dados.

### 4ï¸âƒ£ Garantia de PersistÃªncia e Confiabilidade

Diferente de armazenamentos temporÃ¡rios ou em memÃ³ria, um banco de dados relacional oferece persistÃªncia dos dados de maneira confiÃ¡vel. Isso evita perda de informaÃ§Ãµes em caso de falhas no sistema.

